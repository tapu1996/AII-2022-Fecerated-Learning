{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport keras\nfrom keras import backend as k\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense,Flatten\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import *\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.applications import imagenet_utils\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.models import model_from_json\nfrom keras.layers import Input\nfrom keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential\n#from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\n\n#Dataset\ntrain_path1='../input/emotiondetectiondataset/Expression_dataset/FER dataset/train'\ntest_path1='../input/emotiondetectiondataset/Expression_dataset/FER dataset/test'\n\ntrain_path2='../input/emotiondetectiondataset/Expression_dataset/CK+48/train'\ntest_path2='../input/emotiondetectiondataset/Expression_dataset/CK+48/test'\n\ncombined_train ='../input/emotiondetectiondataset/Expression_dataset/combined/train'\ncombined_test = '../input/emotiondetectiondataset/Expression_dataset/combined/test'\n\nequiTest='../input/equitestfinal/TestSet'\n\nglobal_test = '../input/emotion/FinalTestSet'\n\n# def myFunc(image):\n#     image = np.array(image)\n#     image= np.expand_dims(image, axis=0)\n#     image = image.astype('float32')\n#     image /= 255\n#     return image\n#Splitting dataset\n#train_datagen = ImageDataGenerator(preprocessing_function=keras.applications.inception_resnet_v2.preprocess_input, validation_split=0.2) \n\nfrom keras.models import load_model\n#Change model\nmodel = keras.applications.mobilenet.MobileNet(\n    include_top=True,\n    weights=\"imagenet\",\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n)\nx = model.layers[-2].output\npredictions = Dense(6,activation='softmax')(x)\nlocalmodel1 = Model(inputs=model.input,outputs=predictions)\n\nlocalmodel1.compile(Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n#model.summary()\n#Change 2 lines\npreproce=keras.applications.mobilenet.preprocess_input\nprint(\"dataset1\")\ntrain_batches1= ImageDataGenerator(preprocessing_function=preproce).flow_from_directory(train_path1,target_size=(224,224),batch_size=32,shuffle=True)\ntest_batches1= ImageDataGenerator(preprocessing_function=preproce).flow_from_directory(test_path1,target_size=(224,224),batch_size=32,shuffle=False)\nprint(\"dataset2\")\ntrain_batches2= ImageDataGenerator(preprocessing_function=preproce).flow_from_directory(train_path2,target_size=(224,224),batch_size=32,shuffle=True)\ntest_batches2= ImageDataGenerator(preprocessing_function=preproce).flow_from_directory(test_path2,target_size=(224,224),batch_size=32,shuffle=False)\nprint(\"combined\")\ncombined_train= ImageDataGenerator(preprocessing_function=preproce).flow_from_directory(combined_train,target_size=(224,224),batch_size=32,shuffle=True)\ncombined_test= ImageDataGenerator(preprocessing_function=preproce).flow_from_directory(combined_test,target_size=(224,224),batch_size=32,shuffle=False)\nprint(\"globalTest\")\nglobal_test= ImageDataGenerator(preprocessing_function=preproce).flow_from_directory(global_test,target_size=(224,224),batch_size=32,shuffle=False)\nequi_Test= ImageDataGenerator(preprocessing_function=preproce).flow_from_directory(equiTest,target_size=(224,224),batch_size=32,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T08:40:21.672345Z","iopub.execute_input":"2022-04-28T08:40:21.672779Z","iopub.status.idle":"2022-04-28T08:40:30.284947Z","shell.execute_reply.started":"2022-04-28T08:40:21.672736Z","shell.execute_reply":"2022-04-28T08:40:30.284047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = model.layers[-2].output\npredictions = Dense(6,activation='softmax')(x)\nlocalmodel2 = Model(inputs=model.input,outputs=predictions)\nlocalmodel2.compile(Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T05:39:33.075542Z","iopub.execute_input":"2022-04-28T05:39:33.07591Z","iopub.status.idle":"2022-04-28T05:39:33.108304Z","shell.execute_reply.started":"2022-04-28T05:39:33.07588Z","shell.execute_reply":"2022-04-28T05:39:33.107472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = model.layers[-2].output\npredictions = Dense(6,activation='softmax')(x)\nglobalModel = Model(inputs=model.input,outputs=predictions)\nglobalModel.compile(Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T05:40:29.610439Z","iopub.execute_input":"2022-04-28T05:40:29.610798Z","iopub.status.idle":"2022-04-28T05:40:29.640131Z","shell.execute_reply.started":"2022-04-28T05:40:29.610767Z","shell.execute_reply":"2022-04-28T05:40:29.639358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndef scale_model_weights(weight):\n    '''function for scaling a models weights'''\n    weight_final = []\n    steps = len(weight)\n    for i in range(steps):\n        weight_final.append(0.5 * weight[i])\n    return weight_final\ndef sum_scaled_weights(scaled_weight_list):\n    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n    avg_grad = list()\n    #get the average grad accross all client gradients\n    for grad_list_tuple in zip(*scaled_weight_list):\n        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n        avg_grad.append(layer_mean)\n        \n    return avg_grad","metadata":{"execution":{"iopub.status.busy":"2022-04-28T05:40:33.07042Z","iopub.execute_input":"2022-04-28T05:40:33.070801Z","iopub.status.idle":"2022-04-28T05:40:33.076936Z","shell.execute_reply.started":"2022-04-28T05:40:33.070769Z","shell.execute_reply":"2022-04-28T05:40:33.075938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loss_client1=[]\nval_acc_client1=[]\nval_loss_client2=[]\nval_acc_client2=[]\nval_loss_global1=[]\nval_acc_global1=[]\nval_loss_global2=[]\nval_acc_global2=[]\nval_loss_globalTest=[]\nval_acc_globalTest=[]\n\nfor i in range (25):\n    global_weights = globalModel.get_weights()\n    scaled_local_weight_list = list()\n    localmodel1.set_weights(global_weights)\n    localmodel2.set_weights(global_weights)\n    his = localmodel1.fit(train_batches1,steps_per_epoch=np.ceil(train_batches1.samples / train_batches1.batch_size),epochs =1 ,verbose=1,validation_data = test_batches1,validation_steps=np.ceil(test_batches1.samples / test_batches1.batch_size))\n    loss, acc = localmodel1.evaluate_generator(test_batches1, steps=np.ceil(test_batches1.samples / test_batches1.batch_size), verbose=0)\n    print('client 1 accuracy at ', i ,\":\",acc)\n    val_loss_client1.append(loss)\n    val_acc_client1.append(acc)\n    his = localmodel2.fit(train_batches2,steps_per_epoch=np.ceil(train_batches2.samples / train_batches2.batch_size),epochs =1 ,verbose=1,validation_data = test_batches2,validation_steps=np.ceil(test_batches2.samples / test_batches2.batch_size))\n    loss, acc = localmodel2.evaluate_generator(test_batches2, steps=np.ceil(test_batches2.samples / test_batches2.batch_size), verbose=0)\n    print('client 2 accuracy at ', i ,\":\",acc)\n    val_loss_client2.append(loss)\n    val_acc_client2.append(acc)\n    scaled_weights = scale_model_weights(localmodel1.get_weights())\n    scaled_local_weight_list.append(scaled_weights)\n    scaled_weights = scale_model_weights(localmodel2.get_weights())\n    scaled_local_weight_list.append(scaled_weights)\n    average_weights = sum_scaled_weights(scaled_local_weight_list)\n    #update global model \n    globalModel.set_weights(average_weights)\n    loss, acc = globalModel.evaluate_generator(test_batches1, steps=np.ceil(test_batches1.samples / test_batches1.batch_size), verbose=1)\n    print('Global Model accuracy at 1 ', i ,\":\",acc)\n    val_loss_global1.append(loss)\n    val_acc_global1.append(acc)\n    loss, acc = globalModel.evaluate_generator(test_batches2, steps=np.ceil(test_batches2.samples / test_batches2.batch_size), verbose=1)\n    print('Global Model accuracy at 2 ', i ,\":\",acc)\n    val_loss_global2.append(loss)\n    val_acc_global2.append(acc)\n    loss, acc = globalModel.evaluate_generator(global_test, steps=np.ceil(global_test.samples / global_test.batch_size), verbose=1)\n    print('Global Model accuracy at globatestset ', i ,\":\",acc)\n    val_loss_globalTest.append(loss)\n    val_acc_globalTest.append(acc)\n    avg_acc = (val_acc_global1[i] + val_acc_global2[i] + val_acc_globalTest[i])/3\n    print(\"Average:\",avg_acc)\n    globalModel.save(\"%d-%f.h5\" % (i, val_acc_globalTest[i]))\n    print(\"----------------------------------------------------------\")\n    \ndf = pd.DataFrame(list(zip(val_loss_client1, val_acc_client1,val_loss_client2,val_acc_client2,val_loss_global1,val_acc_global1,val_loss_global2,val_acc_global2,val_loss_globalTest,val_acc_globalTest)),\n               columns =['Loss_client1', 'Acc_client1','Loss_client2','Acc_client2','val_loss_global1','val_acc_global1','val_loss_global2','val_acc_global2','val_loss_globalTest','val_acc_globalTest'])\ndf.to_csv(\"loss_acc.csv\")\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-28T05:40:38.452287Z","iopub.execute_input":"2022-04-28T05:40:38.452632Z","iopub.status.idle":"2022-04-28T06:37:14.309269Z","shell.execute_reply.started":"2022-04-28T05:40:38.452595Z","shell.execute_reply":"2022-04-28T06:37:14.308256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(list(zip(val_loss_client1, val_acc_client1,val_loss_client2,val_acc_client2,val_loss_global1,val_acc_global1,val_loss_global2,val_acc_global2,val_loss_globalTest,val_acc_globalTest)),\n               columns =['Loss_client1', 'Acc_client1','Loss_client2','Acc_client2','val_loss_global1','val_acc_global1','val_loss_global2','val_acc_global2','val_loss_globalTest','val_acc_globalTest'])\ndf.to_csv(\"loss_acc.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:52:23.44421Z","iopub.execute_input":"2022-04-28T06:52:23.444553Z","iopub.status.idle":"2022-04-28T06:52:23.457155Z","shell.execute_reply.started":"2022-04-28T06:52:23.44451Z","shell.execute_reply":"2022-04-28T06:52:23.456211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:52:29.794629Z","iopub.execute_input":"2022-04-28T06:52:29.794986Z","iopub.status.idle":"2022-04-28T06:52:29.814328Z","shell.execute_reply.started":"2022-04-28T06:52:29.794955Z","shell.execute_reply":"2022-04-28T06:52:29.813516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = model.layers[-2].output\npredictions = Dense(3,activation='softmax')(x)\nnew = Model(inputs=model.input,outputs=predictions)\n\nnew.compile(Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:10:39.068264Z","iopub.execute_input":"2022-04-28T07:10:39.068644Z","iopub.status.idle":"2022-04-28T07:10:39.098638Z","shell.execute_reply.started":"2022-04-28T07:10:39.068607Z","shell.execute_reply":"2022-04-28T07:10:39.097796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"localmodel1.save(\"merged.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:55:49.056419Z","iopub.execute_input":"2022-04-28T09:55:49.056763Z","iopub.status.idle":"2022-04-28T09:55:49.345323Z","shell.execute_reply.started":"2022-04-28T09:55:49.056731Z","shell.execute_reply":"2022-04-28T09:55:49.344383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"his = localmodel1.fit(combined_train,steps_per_epoch=np.ceil(combined_train.samples / combined_train.batch_size),epochs =1 ,verbose=1,validation_data = combined_test,validation_steps=np.ceil(combined_test.samples / combined_test.batch_size))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:37:00.948968Z","iopub.execute_input":"2022-04-28T09:37:00.949295Z","iopub.status.idle":"2022-04-28T09:38:49.372149Z","shell.execute_reply.started":"2022-04-28T09:37:00.949264Z","shell.execute_reply":"2022-04-28T09:38:49.371301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, acc = new.evaluate_generator(test_batches1, steps=np.ceil(test_batches1.samples / test_batches1.batch_size), verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T03:30:48.164321Z","iopub.execute_input":"2022-04-27T03:30:48.164668Z","iopub.status.idle":"2022-04-27T03:31:08.8631Z","shell.execute_reply.started":"2022-04-27T03:30:48.164637Z","shell.execute_reply":"2022-04-27T03:31:08.862241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc","metadata":{"execution":{"iopub.status.busy":"2022-04-27T03:31:13.975977Z","iopub.execute_input":"2022-04-27T03:31:13.976341Z","iopub.status.idle":"2022-04-27T03:31:13.984311Z","shell.execute_reply.started":"2022-04-27T03:31:13.976308Z","shell.execute_reply":"2022-04-27T03:31:13.983488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generate training curve\nimport matplotlib.pyplot as plt\nimport pandas as pd\nhistory = his\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\ndf = pd.DataFrame(list(zip(train_acc, val_acc,train_loss,val_loss)),\n               columns =['Training Acc', 'Validation Acc','Training Loss','Validation Loss'])\ndf.to_csv('curve.csv')\nepochs = range(1,26)\nplt.plot(epochs, train_acc, 'g', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n#plt.title('Attention LSTM-CNN (FastText Embedding)')\nplt.xlabel('Epochs', fontsize='medium')\nplt.ylabel('Accuracy', fontsize='medium')\nplt.legend()\n#sn.set(font_scale=1)\nplt.savefig('train.pdf', format='pdf', dpi=300)\nplt.savefig('train.png', format='png', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:30:45.436818Z","iopub.execute_input":"2022-04-28T09:30:45.437151Z","iopub.status.idle":"2022-04-28T09:30:45.92254Z","shell.execute_reply.started":"2022-04-28T09:30:45.437119Z","shell.execute_reply":"2022-04-28T09:30:45.921694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generate training curve\n#import matplotlib.pyplot as plt\n#history = his\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nepochs = range(1,51)\nplt.plot(epochs, train_acc, 'g', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n#plt.title('Attention LSTM-CNN (FastText Embedding)')\nplt.xlabel('Epochs', fontsize='medium')\nplt.ylabel('Accuracy', fontsize='medium')\nplt.legend()\n#sn.set(font_scale=1)\nplt.savefig('train.pdf', format='pdf', dpi=300)\nplt.savefig('train.png', format='png', dpi=300)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"local","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generate Result\nfrom __future__ import print_function\nimport sklearn\nfrom matplotlib import pyplot as plt \nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport h5py\nimport os\nimport json\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Change HERE\n#model = load_model('./17-0.678674.h5')\nmodel=localmodel1\n#combined_test\n#equi_Test\ntest_batches=equi_Test\nloss, acc = model.evaluate_generator(test_batches, steps=np.ceil(test_batches.samples / test_batches.batch_size), verbose=1)\nprint('accuracy:',acc)\npredictions = model.predict_generator(test_batches, steps = np.ceil(test_batches.samples / test_batches.batch_size), verbose=1, workers=0) \nY_pred = np.argmax(predictions, axis=1) \nprint('Classification Report') \nprint(classification_report(test_batches.classes, Y_pred))\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\noriginal = test_batches.labels\ncm=confusion_matrix(original,Y_pred)\nprint(cm)\n#y_true = [\"0\",\"1\",\"2\"]\ny_true=['Anger', 'Disgust','Fear', 'Happy','Sad','Surprise']\ndata = cm\n# class1_acc = data[0][0]/(data[0][0]+data[0][1]+data[0][2])\n# class2_acc = data[1][1]/(data[1][0]+data[1][1]+data[1][2])\n# class3_acc = data[2][2]/(data[2][0]+data[2][1]+data[2][2])\n# print('class-1 acc: ',class1_acc)\n# print('class-2 acc: ',class2_acc)\n# print('class-3 acc: ',class3_acc)\nfor i in range(0,6):\n    acc = data[i][i]\n    s =0\n    for j in range(0,6):\n        s = s+data[i][j]\n    acc = acc/s\n    print(\"Class %d accuracy - %f\" % (i, acc))\n\n\ndf_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n#df_cm.index.name = 'Actual'\n#df_cm.columns.name = 'Predicted'\nsn.set(font_scale=1)#for label size\nsn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 18}, fmt=\"d\")\nplt.savefig('Cm.pdf', format='pdf', dpi=300)\nplt.savefig('Cm.png', format='png', dpi=300)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:52:16.183517Z","iopub.execute_input":"2022-04-28T09:52:16.18387Z","iopub.status.idle":"2022-04-28T09:52:17.876244Z","shell.execute_reply.started":"2022-04-28T09:52:16.183839Z","shell.execute_reply":"2022-04-28T09:52:17.875344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}